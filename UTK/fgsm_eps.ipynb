{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessor import _imread as imread\n",
    "from preprocessor import _imresize as imresize\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.losses import categorical_crossentropy\n",
    "import argparse\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'male', 1: 'female'},\n",
       " {'female': 1, 'male': 0},\n",
       " {0: 'white', 1: 'black', 2: 'asian', 3: 'indian', 4: 'others'},\n",
       " {'asian': 2, 'black': 1, 'indian': 3, 'others': 4, 'white': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data/UTKFace\"\n",
    "TRAIN_TEST_SPLIT = 0.7\n",
    "IM_WIDTH = IM_HEIGHT = 198\n",
    "input_shape = (IM_WIDTH, IM_HEIGHT, 3)\n",
    "ID_GENDER_MAP = {0: 'male', 1: 'female'}\n",
    "GENDER_ID_MAP = dict((g, i) for i, g in ID_GENDER_MAP.items())\n",
    "ID_RACE_MAP = {0: 'white', 1: 'black', 2: 'asian', 3: 'indian', 4: 'others'}\n",
    "RACE_ID_MAP = dict((r, i) for i, r in ID_RACE_MAP.items())\n",
    "\n",
    "ID_GENDER_MAP, GENDER_ID_MAP, ID_RACE_MAP, RACE_ID_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filepath(filepath):\n",
    "    try:\n",
    "        path, filename = os.path.split(filepath)\n",
    "        filename, ext = os.path.splitext(filename)\n",
    "        age, gender, race, _ = filename.split(\"_\")\n",
    "        return int(age), ID_GENDER_MAP[int(gender)], ID_RACE_MAP[int(race)]\n",
    "    except Exception as e:\n",
    "        print(filepath)\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UTKFace/61_1_20170109142408075.jpg.chip.jpg\n",
      "data/UTKFace/39_1_20170116174525125.jpg.chip.jpg\n",
      "data/UTKFace/61_1_20170109150557335.jpg.chip.jpg\n"
     ]
    }
   ],
   "source": [
    "# create a pandas data frame of images, age, gender and race\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"*.jpg\"))\n",
    "attributes = list(map(parse_filepath, files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>data/UTKFace/28_1_0_20170116164219746.jpg.chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>data/UTKFace/24_1_0_20170117150731090.jpg.chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>data/UTKFace/24_1_1_20170113003752421.jpg.chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.0</td>\n",
       "      <td>female</td>\n",
       "      <td>indian</td>\n",
       "      <td>data/UTKFace/26_1_3_20170104235148954.jpg.chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>data/UTKFace/51_0_0_20170117190825002.jpg.chip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender    race                                               file\n",
       "0  28.0  female   white  data/UTKFace/28_1_0_20170116164219746.jpg.chip...\n",
       "1  24.0  female   white  data/UTKFace/24_1_0_20170117150731090.jpg.chip...\n",
       "3  24.0  female   black  data/UTKFace/24_1_1_20170113003752421.jpg.chip...\n",
       "4  26.0  female  indian  data/UTKFace/26_1_3_20170104235148954.jpg.chip...\n",
       "5  51.0    male   white  data/UTKFace/51_0_0_20170117190825002.jpg.chip..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(attributes)\n",
    "df['file'] = files\n",
    "df.columns = ['age', 'gender', 'race', 'file']\n",
    "df = df.dropna()\n",
    "df = df[(df['age'] > 10) & (df['age'] < 65)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, input_shape):\n",
    "    image = imresize(image, input_shape[:2])\n",
    "    image = image.astype('float32')\n",
    "    image = image/255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "def predict(model, image):\n",
    "    predictions = model.predict(image)\n",
    "    return predictions\n",
    "\n",
    "def FGSM(x, model):\n",
    "    sess = K.get_session()\n",
    "    x_adv1 = x\n",
    "    x_adv2 = x\n",
    "    x_adv3 = x\n",
    "    x_adv4 = x\n",
    "    x_adv5 = x\n",
    "    alpha_1 = 1.\n",
    "    alpha_2 = -1.\n",
    "    #print(list(map(lambda x: x.name, model.layers)))\n",
    "    # dense7 -> layer before race output\n",
    "    # dense8 -> layer before gender output\n",
    "    dense_7 = model.get_layer('dense_7').output\n",
    "    dense_7_grads = K.gradients(dense_7, model.input)\n",
    "\n",
    "    dense_8 = model.get_layer('dense_8').output\n",
    "    dense_8_grads = K.gradients(dense_8, model.input)\n",
    "\n",
    "    final_grads = tf.constant(alpha_1) * dense_7_grads + tf.constant(alpha_2) * dense_8_grads\n",
    "    #grads = K.gradients(final_grads, model.input)\n",
    "    delta = K.sign(final_grads[0])\n",
    "    x_adv1 = x_adv1 + 0.1 * delta\n",
    "    x_adv2 = x_adv2 + 0.2 * delta\n",
    "    x_adv3 = x_adv3 + 0.3 * delta\n",
    "    x_adv4 = x_adv4 + 0.4 * delta\n",
    "    x_adv5 = x_adv5 + 0.5 * delta\n",
    "    \n",
    "    x_adv1 = K.clip(x_adv1, 0.0 ,1.0)\n",
    "    x_adv2 = K.clip(x_adv2, 0.0 ,1.0)\n",
    "    x_adv3 = K.clip(x_adv3, 0.0 ,1.0)\n",
    "    x_adv4 = K.clip(x_adv4, 0.0 ,1.0)\n",
    "    x_adv5 = K.clip(x_adv5, 0.0 ,1.0)\n",
    "\n",
    "    gradients, x_adv1_array, x_adv2_array, x_adv3_array, x_adv4_array, x_adv5_array = sess.run([final_grads,\n",
    "                                                                                                x_adv1,\n",
    "                                                                                                x_adv2,\n",
    "                                                                                                x_adv3,\n",
    "                                                                                                x_adv4,\n",
    "                                                                                                x_adv5],\n",
    "                                                                                                feed_dict={model.input:x})\n",
    "    #print('GRADIENT SUM:{}'.format(np.sum(gradients[0])))\n",
    "    return x_adv1_array, x_adv2_array, x_adv3_array, x_adv4_array, x_adv5_array\n",
    "\n",
    "def plot_adversarial(img_list):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    eps = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    for n, img in enumerate(img_list):\n",
    "        ax = plt.subplot(2,3,n+1)\n",
    "        ax.set_title('Epsilon: {}'.format(eps[n]))\n",
    "        plt.imshow(img[0])\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model_path = \"./models/VGG16_adv_model.h5\"\n",
    "clean_model_path = \"./models/VGG16_clean_model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_samples(x, model):\n",
    "    adv_sample = FGSM(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_adv_samples(generator, model):\n",
    "    folder = \"adv_generated_imgs\"\n",
    "    for data in generator:\n",
    "        samples, labels, filenames = data\n",
    "        # running loop over a batch\n",
    "        prediction = predict(model, samples)\n",
    "        adv_samples = FGSM(samples, model)\n",
    "        for index in range(len(samples)):\n",
    "            filename = filenames[index]\n",
    "\n",
    "            final_name = \"{}/{}/{}/{}\".format(filename.split(\"/\")[0], folder, \"1\", filename.split(\"/\")[-1])\n",
    "            img = Image.fromarray(np.uint8(adv_samples[0][index]*255))\n",
    "            img.save(final_name)\n",
    "\n",
    "            final_name = \"{}/{}/{}/{}\".format(filename.split(\"/\")[0], folder, \"2\", filename.split(\"/\")[-1])\n",
    "            img = Image.fromarray(np.uint8(adv_samples[1][index]*255))\n",
    "            img.save(final_name)\n",
    "\n",
    "            final_name = \"{}/{}/{}/{}\".format(filename.split(\"/\")[0], folder, \"3\", filename.split(\"/\")[-1])\n",
    "            img = Image.fromarray(np.uint8(adv_samples[2][index]*255))\n",
    "            img.save(final_name)\n",
    "\n",
    "            final_name = \"{}/{}/{}/{}\".format(filename.split(\"/\")[0], folder, \"4\", filename.split(\"/\")[-1])\n",
    "            img = Image.fromarray(np.uint8(adv_samples[3][index]*255))\n",
    "            img.save(final_name)\n",
    "\n",
    "            final_name = \"{}/{}/{}/{}\".format(filename.split(\"/\")[0], folder, \"5\", filename.split(\"/\")[-1])\n",
    "            img = Image.fromarray(np.uint8(adv_samples[4][index]*255))\n",
    "            img.save(final_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "\n",
    "def get_data_generator(df, indices, for_training, batch_size=16):\n",
    "    # filenames variable is not being passed right now for the evaluation but would be\n",
    "    # required if we want to generate the dataset of adversarial images\n",
    "    images, ages, races, genders, filenames = [], [], [], [], []\n",
    "    while True:\n",
    "        for i in indices:\n",
    "            r = df.iloc[i]\n",
    "            file, age, race, gender = r['file'], r['age'], r['race_id'], r['gender_id']\n",
    "            im = Image.open(file)\n",
    "            im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "            im = np.array(im) / 255.0\n",
    "            # filenames.append(file)\n",
    "            images.append(im)\n",
    "            ages.append(age / max_age)\n",
    "            races.append(to_categorical(race, len(RACE_ID_MAP)))\n",
    "            genders.append(to_categorical(gender, 2))\n",
    "            if len(images) >= batch_size:\n",
    "                yield np.array(images), [np.array(ages), np.array(races), np.array(genders)] #, filenames\n",
    "                images, ages, races, genders, filenames = [], [], [], [], []\n",
    "        if not for_training:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed is not common with model training on the other notebook\n",
    "p = np.random.permutation(len(df))\n",
    "train_up_to = int(len(df) * TRAIN_TEST_SPLIT)\n",
    "test_idx = p[train_up_to:]\n",
    "\n",
    "df['gender_id'] = df['gender'].map(lambda gender: GENDER_ID_MAP[gender])\n",
    "df['race_id'] = df['race'].map(lambda race: RACE_ID_MAP[race])\n",
    "\n",
    "max_age = df['age'].max()\n",
    "\n",
    "test_gen = get_data_generator(df, test_idx, for_training=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0923 08:42:15.419989 139653548660544 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0923 08:42:19.614129 139653548660544 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(adv_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_all_adv_samples(test_gen, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate clean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_data_generator(df, indices, eps, for_training=False, batch_size=128):\n",
    "    images, ages, races, genders = [], [], [], []\n",
    "    folder = \"adv_generated_imgs\"\n",
    "    while True:\n",
    "        for i in indices:\n",
    "            r = df.iloc[i]\n",
    "            file, age, race, gender = r['file'], r['age'], r['race_id'], r['gender_id']\n",
    "            filename = \"{}/{}/{}/{}\".format(file.split(\"/\")[0], folder, str(eps), file.split(\"/\")[-1])\n",
    "            im = Image.open(filename)\n",
    "            im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "            im = np.array(im) / 255.0\n",
    "            images.append(im)\n",
    "            ages.append(age / max_age)\n",
    "            races.append(to_categorical(race, len(RACE_ID_MAP)))\n",
    "            genders.append(to_categorical(gender, 2))\n",
    "            if len(images) >= batch_size:\n",
    "                yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n",
    "                images, ages, races, genders = [], [], [], []\n",
    "        if not for_training:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean model\n",
    "model_path = \"./models/VGG16_clean_model.h5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.014563413336873055,\n",
       " u'age_output_mae': 0.09220802783966064,\n",
       " u'gender_output_accuracy': 0.9751090407371521,\n",
       " u'gender_output_loss': 0.10134270042181015,\n",
       " 'loss': 0.5926063060760498,\n",
       " u'race_output_accuracy': 0.8860828280448914,\n",
       " u'race_output_loss': 0.46940693259239197}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eps = 0\n",
    "test_gen = get_data_generator(df, test_idx, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.027471547946333885,\n",
       " u'age_output_mae': 0.13309116661548615,\n",
       " u'gender_output_accuracy': 0.8731831312179565,\n",
       " u'gender_output_loss': 0.3502197563648224,\n",
       " 'loss': 2.1239891052246094,\n",
       " u'race_output_accuracy': 0.565588653087616,\n",
       " u'race_output_loss': 2.089019775390625}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1\n",
    "test_gen = get_adv_data_generator(df, test_idx, eps, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.03649305924773216,\n",
       " u'age_output_mae': 0.1581742763519287,\n",
       " u'gender_output_accuracy': 0.7738008499145508,\n",
       " u'gender_output_loss': 0.5750532746315002,\n",
       " 'loss': 3.5959367752075195,\n",
       " u'race_output_accuracy': 0.4502180218696594,\n",
       " u'race_output_loss': 3.2963874340057373}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 2\n",
    "test_gen = get_adv_data_generator(df, test_idx, eps, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.04659625515341759,\n",
       " u'age_output_mae': 0.1837593913078308,\n",
       " u'gender_output_accuracy': 0.6500726938247681,\n",
       " u'gender_output_loss': 0.8763740062713623,\n",
       " 'loss': 4.840598106384277,\n",
       " u'race_output_accuracy': 0.4075218141078949,\n",
       " u'race_output_loss': 4.217748641967773}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 3\n",
    "test_gen = get_adv_data_generator(df, test_idx, eps, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.05641218647360802,\n",
       " u'age_output_mae': 0.20550322532653809,\n",
       " u'gender_output_accuracy': 0.5803052186965942,\n",
       " u'gender_output_loss': 1.1167323589324951,\n",
       " 'loss': 5.559659481048584,\n",
       " u'race_output_accuracy': 0.403888076543808,\n",
       " u'race_output_loss': 4.7010817527771}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 4\n",
    "test_gen = get_adv_data_generator(df, test_idx, eps, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'age_output_loss': 0.062024276703596115,\n",
       " u'age_output_mae': 0.2172510027885437,\n",
       " u'gender_output_accuracy': 0.5432412624359131,\n",
       " u'gender_output_loss': 1.264703631401062,\n",
       " 'loss': 5.984292030334473,\n",
       " u'race_output_accuracy': 0.4035246968269348,\n",
       " u'race_output_loss': 4.879197120666504}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 5\n",
    "test_gen = get_adv_data_generator(df, test_idx, eps, for_training=False, batch_size=128)\n",
    "dict(zip(model.metrics_names, model.evaluate_generator(test_gen, steps=len(test_idx)//128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
